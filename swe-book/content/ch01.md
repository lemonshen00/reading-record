# What Is Software Engineering?
> Nothing is built on stone; all is built on sand, but we must build as if the sand were stone. ----Jorge Luis Borges

We see three critical differences between programming and software engineering: time, `scale`, and the `trade-offs` at play.`On` a software engineering project, engineers need to `be more concerned with` the `passage of time` and the eventual need for change. `In` a software engineering organization, we need to `be more concerned about` scale and `efficiency`, both for the software we produce as well as for the organization that is producing it. Finally, as software engineers, we are asked to make more complex decisions with `higher-stakes outcomes`, often based on `imprecise(precise:精准的) estimates` of time and growth.

`Within` Google, we sometimes say, `Software engineering is programming integrated over time.` Programming is certainly a `significant` part of software engineering: after all, programming is how you generate new software `in the first place`. If you accept this `distinction`, it also becomes clear that we might need to `delineate` between programming tasks (development) and software engineering tasks (development, `modification`, `maintenance`). The addition of time adds an important new `dimension` to programming. `Cubes` aren’t `squares`, distance isn’t `velocity`. Software engineering isn’t programming.

One way to see the `impact` of time on a program is to think about the question, “What is the expected life span of your code?” Reasonable answers `to` this question vary by roughly a factor of 100,000. It is just as reasonable to think of code that needs to last for a few minutes as it is to imagine code that will `live for decades`. Generally, code `on the short end of that spectrum` is unaffected by time. It is `unlikely` that you need to adapt to a new version of your `underlying libraries`, operating system (OS), hardware, or language version for a program whose `utility` `spans` only an hour. These `short-lived` systems are effectively “just” a programming problem, `in the same way` that `a cube compressed far enough in one dimension is a square`. As we expand that time to allow for longer life spans, change becomes more important. `Over a span of` a decade or more, most program dependencies, whether `implicit` or `explicit`, will likely change. This `recognition` is at the root of our distinction between software engineering and programming.

This distinction is at the core of what we call `sustainability` for software. Your project is `sustainable` if, for the expected life span of your software, `you are capable of` reacting to whatever valuable change `comes along`, for either technical or business reasons.  Importantly, we are looking only for capability—you might choose not to `perform a given upgrade`, either for `lack of value` or other `priorities`. When you are `fundamentally` `incapable of` reacting to a change in underlying technology or product direction, you’re placing a high-risk bet on the hope that such a change never becomes critical. For short-term projects, that might be a safe `bet`. Over `multiple` decades, it probably isn’t.

Another way to look at software engineering is to consider scale.  How many people are involved? What part do they play in the development and maintenance over time? A programming task is often an act of `individual creation`, but a software engineering task is a team effort. An early attempt to define software engineering produced a good definition for this viewpoint: `The multiperson development of multiversion programs.` This suggests the difference between software engineering and programming is one of both time and people. `Team collaboration` presents new problems, but also provides more `potential` to produce valuable systems than any single programmer could.

Team organization, project `composition`, and the policies and practices of a software project all `dominate` this `aspect` of software engineering `complexity`. These problems are `inherent` to scale: as the organization grows and its projects expand, does it become more efficient at producing software? Does our development workflow become more efficient as we grow, or do our `version control policies` and testing `strategies` cost us proportionally more? Scale issues around communication and human scaling have been discussed since the early days of software engineering, going all the way back to the Mythical Man Month Such scale issues are often matters of policy and are fundamental to the question of software sustainability: how much will it cost to do the things that we need to do repeatedly? 

We can also say that software engineering is different from programming `in terms of` the complexity of decisions that need to be made and their `stakes`. In software engineering, we are regularly forced to `evaluate` the trade-offs between several paths forward, sometimes with high stakes and often with `imperfect` value metrics. The job of a software engineer, or a software engineering leader, is to aim for sustainability and management of the scaling costs for the organization, the product, and the development workflow. With those inputs in mind, `evaluate` your trade-offs and make `rational` decisions. We might sometimes `defer maintenance changes`, or even `embrace` policies that don’t scale well, with the knowledge that we’ll need to revisit those decisions. Those choices should be `explicit` and clear about the deferred costs.

Rarely is there a `one-size-fits-all(翻译：一刀切，绝了)` solution in software engineering, and the same `applies to` this book. Given a factor of 100,000 for reasonable answers on “How long will this software live,” a range of perhaps a factor of 10,000 for “How many engineers are in your organization,” and who-knows-how-much for “How many compute resources are available for your project,” Google’s experience will probably not match yours. In this book, we aim to present what we’ve found that works for us in the construction and maintenance of software that we expect to last for decades, with tens of thousands of engineers, and world-spanning compute resources. Most of the practices that we find are necessary at that scale will also work well for `smaller endeavors`: consider this a report on one engineering ecosystem that we think could be good as you `scale up`. In a few places, `super-large scale` comes with its own costs, and we’d be happier to `not be paying extra overhead`. We call those out as a warning. Hopefully if your organization grows large enough to be worried about those costs, you can find a better answer.

Before we get to `specifics` about teamwork, culture, policies, and tools, let’s first elaborate on these primary themes of time, scale, and trade-offs.

## Time and Change
When a `novice` is learning to program, the life span of the resulting code is usually measured in hours or days. Programming `assignments` and exercises tend to be write-once, with little to no `refactoring` and certainly no long-term maintenance. These programs are often not rebuilt or `executed` ever again after their `initial` production. This isn’t surprising in a `pedagogical` setting. Perhaps in secondary or post-secondary education, we may find a team project course or hands-on `thesis`. If so, such projects are likely the only time student code will live longer than a month or so. Those developers might need to `refactor some code`, perhaps as a response to changing requirements(需求变了), but it is unlikely they are being asked to deal with `broader` changes to their environment.

We also find developers of short-lived code in common industry settings. `Mobile apps` often have a `fairly` short life span(Nothing is Certain Except Death, Taxes and a Short Mobile App Lifespan), and for better or worse, full rewrites are `relatively` common. Engineers at an early-stage startup might rightly choose to focus on immediate goals over long-term investments: the company might not live long enough to `reap the benefits` of an `infrastructure` investment that pays off slowly. A serial startup developer could very reasonably have 10 years of development experience and little or no experience `maintaining` any piece of software expected to exist for longer than a year or two.

On the other end of the spectrum, some successful projects have an effectively `unbounded` life span: we can’t reasonably `predict` an endpoint for Google Search, the Linux kernel, or the Apache HTTP Server project.  For most Google projects, we must `assume` that they will live `indefinitely`—we cannot predict when we won’t need to upgrade our dependencies, language versions, and so on. As their lifetimes grow, these long-lived projects eventually have a different feel to them than programming assignments or startup development.

Consider Figure 1-1, which `demonstrates` two software projects `on opposite ends` of this “expected life span” spectrum.  For a programmer working on a task with an expected life span of hours, what types of maintenance are reasonable to expect? That is, if a new version of your OS comes out while you’re working on a Python script that will be `executed` one time, should you drop what you’re doing and upgrade? Of course not: the upgrade is not critical. But on the opposite end of the spectrum, Google Search being `stuck` on a version of our OS from the 1990s would be a clear problem.

![image](https://user-images.githubusercontent.com/13763576/226499372-fdc723ea-d4c4-4971-a488-4cdf401c9e85.png)

The low and high points on the expected life span spectrum suggest that there’s a `transition` somewhere. Somewhere along the line between a `one-off program` and a project that lasts for decades, a transition happens: a project must begin to `react to` changing externalities. For any project that didn’t plan for upgrades from the start, that `transition` is likely very painful for three reasons, each of which `compounds` the others:

- You’re performing a task that hasn’t yet been done for this project; more `hidden assumptions` have been baked-in.
- The engineers trying to do the upgrade are less likely to have experience in this sort of task.
- The size of the upgrade is often larger than usual, doing several years’ worth of upgrades at once `instead of` a more `incremental` upgrade.

And thus, after actually going through such an upgrade once (or `giving up part way through`), it’s pretty reasonable to `overestimate` the cost of doing a subsequent upgrade and decide “Never again.” Companies that come to this conclusion end up committing to just throwing things out and rewriting their code, or deciding to never upgrade again. Rather than take the natural approach by avoiding a painful task, sometimes the more responsible answer is to invest in making it less painful. It all depends on the cost of your upgrade, the value it provides, and the expected life span of the project in question.

Getting through not only that first big upgrade, but getting to the point at which you can `reliably` stay current `going forward`, is the `essence` of long-term sustainability for your project. Sustainability requires planning and managing the impact of required change. For many projects at Google, we believe we have achieved this sort of sustainability, `largely through trial and error`.

So, concretely, how does short-term programming differ from producing code with a much longer expected life span? Over time, we need to be much more aware of the difference between “happens to work” and “is maintainable.” There is no perfect solution for identifying these issues. That is unfortunate, because keeping software maintainable for the long-term is a `constant` `battle`.

Hyrum’s Law

If you are maintaining a project that is used by other engineers, the most important lesson about “it works” versus “it is maintainable” is what we’ve come to call Hyrum’s Law:

> With a `sufficient` number of users of an API, it does not matter what you promise in the contract: all `observable` behaviors of your system will be depended on by somebody.

In our experience, this `axiom` is a `dominant factor` in any discussion of changing software over time. It is conceptually akin to entropy: discussions of change and maintenance over time must be aware of Hyrum’s Law just as discussions of efficiency or thermodynamics must be mindful of entropy. Just because entropy never `decreases`(increase 增长) doesn’t mean we shouldn’t try to be efficient. Just because Hyrum’s Law will apply when maintaining software doesn’t mean we can’t plan for it or try to better understand it. We can `mitigate` it, but we know that it can never be `eradicated`.

Hyrum’s Law represents the `practical knowledge` that—even with the best of `intentions`, the best engineers, and `solid practices` for code review—we cannot assume perfect `adherence` to published contracts or best practices. As an API owner, you will gain some `flexibility` and freedom by being clear about interface promises, but in practice, the `complexity` and difficulty of a given change also depends on how useful a user finds some `observable` behavior of your API. If users cannot depend on such things, your API will be easy to change. Given enough time and enough users, even the most innocuous change will break something;9 your analysis of the value of that change must `incorporate` the difficulty in investigating, identifying, and resolving those `breakages`.

Example: Hash Ordering
Consider the example of hash iteration ordering. If we insert five elements into a hash-based set, in what order do we get them out?

Most programmers know that hash tables are non-obviously ordered. Few know the `specifics` of whether the particular hash table they are using is intending to provide that particular ordering forever. This might seem `unremarkable`, but over the past decade or two, the computing industry’s experience using such types has evolved:

- Hash flooding attacks provide an increased incentive for `nondeterministic(deterministic:确定的)` hash iteration(iterator:迭代器).
- Potential efficiency gains from research into improved hash algorithms or hash containers require changes to hash iteration order.
- Per Hyrum’s Law, programmers will write programs that depend on the order in which a hash table is traversed, if they have the ability to do so.

As a result, if you ask any expert “Can I assume a `particular` output sequence for my hash container?” that expert will presumably say “No.” `By and large `that is correct, but perhaps `simplistic`(simple:简单的). `A more nuanced answer is`, “If your code is short-lived, with no changes to your hardware, language runtime, or choice of data structure, such an assumption is fine. If you don’t know how long your code will live, or you cannot promise that nothing you depend upon will ever change, such an assumption is incorrect.” `Moreover`, even if your own implementation does not depend on hash container order, it might be used by other code that implicitly(implicite:隐式的/explicate:显式的) creates such a dependency. For example, if your library serializes values into a Remote Procedure Call (RPC) response, the RPC caller might wind up depending on the order of those values.

This is a very basic example of the difference between “it works” and “it is correct.” For a short-lived program, depending on the `iteration(iterator:迭代器) order` of your containers will not cause any technical problems. For a software engineering project, on the other hand, such reliance on a defined order is a risk—given enough time, something will make it valuable to change that iteration order. That value can `manifest` in a number of ways, be it efficiency, security, or merely future-proofing the data structure to allow for future changes. When that value becomes clear, you will need to `weigh the trade-offs` `between that value and` the pain of breaking your developers or customers.

Some languages specifically `randomize` hash ordering between library versions or even between `execution` of the same program in an attempt to prevent dependencies. But even this still allows for some Hyrum’s Law surprises: there is code that uses hash iteration ordering as an inefficient random-number generator. Removing such `randomness` now would break those users. Just as entropy increases in every thermodynamic system, Hyrum’s Law applies to every observable behavior.

Thinking over the differences between code written with a “works now” and a “works indefinitely” `mentality`, we can `extract` some clear relationships. Looking at code as an `artifact` with a (highly) variable lifetime requirement, we can begin to `categorize` programming styles: code that depends on brittle and unpublished features of its dependencies is likely to be described as “hacky” or “clever,” whereas code that follows best practices and has planned for the future is more likely to be described as “clean” and “maintainable.” Both have their purposes, but which one you select depends crucially on the expected life span of the code in question. We’ve taken to saying, “It’s programming if 'clever' is a `compliment`, but it’s software engineering if 'clever' is an `accusation`.”

Why Not Just Aim for “Nothing Changes”?
Implicit in all of this discussion of time and the need to react to change is the `assumption` that change might be necessary. Is it?

As with effectively everything else in this book, it depends.  We’ll readily `commit to` “For most projects, over a long enough time period, everything `underneath` them might need to be changed.” If you have a project written in pure C with no `external` dependencies (or only external dependencies that promise great long-term stability, like POSIX), you might well be able to avoid any form of refactoring or difficult upgrade. C does a great job of providing stability—in many respects, that is its primary purpose.

Most projects have far more `exposure` to shifting underlying technology. Most programming languages and runtimes change much more than C does. Even libraries implemented in pure C might change to support new features, which can affect downstream users.  Security problems are disclosed in all manner of technology, from processors to networking libraries to application code. Every piece of technology upon which your project depends has some (hopefully small) risk of containing critical bugs and security vulnerabilities that might come to light only after you’ve started relying on it. If you are incapable of deploying a patch for Heartbleed or mitigating speculative execution problems like Meltdown and Spectre because you’ve assumed (or promised) that nothing will ever change, that is a significant `gamble`. 
